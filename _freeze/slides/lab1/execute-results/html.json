{
  "hash": "25074a9c07ba0f3fbc9332fb8940b644",
  "result": {
    "markdown": "---\ntitle: \"Maximum likelihood estimator\"\n# date: \"September 11, 2023\"\nformat: \n    revealjs:\n      smaller: true\n---\n\n\n# Background: likelihoods\n\n\n::: {.cell}\n\n:::\n\n\n\n## Example: normal likelihood\n\nLet $X$ be the resting heart rate (RHR) in beats per minute of a student in this class.\n\nAssume RHR is normally distributed with some mean $\\mu$ and standard deviation $8$.\n\n. . .\n\n\n$$\n\\textbf{Data-generative model: } X_i \\overset{\\mathrm{iid}}{\\sim} N(\\mu, 64)\n$$\n\n. . .\n\nIf we observe three student heart rates, {75, 58, 68} then our likelihood \n\n$$L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).$$\n\nThat is, the joint density function of the observed data, viewed as a function of the parameter.\n\n. . . \n\n::: callout-important\nThe likelihood itself is **not a density function**. The integral with respect to the parameter does not need to equal 1.\n:::\n\n## Visualizing the likelihood\n\n\n$$L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).$$\n\n\n::: panel-tabset\n\n### data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = c(75, 58, 68)\n```\n:::\n\n\n\n### likelihood function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL = function(mu, x) {\n  stopifnot(is.numeric(x))\n  n = length(x)\n  likelihood = 1\n  for(i in 1:n){\n    likelihood = likelihood * dnorm(x[i], mean = mu, sd = 8)\n  }\n  return(likelihood)\n}\n```\n:::\n\n\n### plot\n\n\n::: {.cell layoutWidth='25'}\n::: {.cell-output-display}\n![](lab1_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n### plot code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  xlim(c(50, 83)) +\n  geom_function(fun = L, args = list(x = x)) +\n  theme_bw() +\n  labs(x = expression(mu), y = \"likelihood\") + \n  geom_vline(xintercept = 67, color = 'red')\n```\n:::\n\n\n:::\n\n. . . \n\nThe maximum likelihood estimate $\\hat{\\mu} = \\frac{75 + 58 + 68}{3} = 67$.\n\nThe **maximum likelihood estimate** is the parameter value that *maximizes* the likelihood function. \n\n\n::: {.cell}\n\n:::\n\n\n## The log-likelihood\n\nNotice how small the y-axis is on the previous slide. What happens to the scale of the likelihood as we add additional data points?\n\n\n\n$$\nL(\\mu) = \\prod_{i = 1}^{n} f_x(x_i |\\mu)\n$$\n\n\n. . . \n\nSince densities often evaluate between 0 and 1, multiplying many together (as we usually do in likelihoods) can quickly result in floating point underflow. That is, numbers smaller than the computer can actually represent in memory.\n\n- Note: sometimes densities evaluate to greater than 1 (e.g. `dnorm(0, 0, 0.001)`) and multiplying several together can result in *overflow*.\n\n. . .\n\n#### log to the rescue!\n\n- `log` is a monotonic function, i.e. $x > y$ implies $\\log(x) > \\log(y)$, because of this the maximum of $f$ is the same as the maximum of $\\log f$.\n\n- additionally, `log` turns products into sums\n\nin practice, we always work with the log-likelihood,\n\n\n$$\n\\log L(\\mu) = \\sum_{i = 1}^n \\log f_x(x_i | \\mu).\n$$\n\n# MLE\n\n## Maximum likelihood estimation (MLE)\n\nHow did we know to take the average of the values to find the maximum likelihood estimator $\\hat{\\mu}$?\n\n. . .\n\nFrom calculus, we know that to maximize a function, we need to find where the slope equals zero (technically, to ensure we find some maxima and not a minima we need to also check that the second derivative is negative). \n\n### Example: normal likelihood\n\nFor the normal likelihood example on the previous slide, we can see visually that the function is concave.\n\nTo find the maximum,\n\n\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu} \\log L(\\mu) &= \\sum_{i}\\frac{d}{d\\mu} \\log f_x(x_i |\\mu)\\\\ \n&= \\sum_{i}\\frac{d}{d\\mu} \\left[ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right]\\\\\n&= \\sum_i \\frac{1}{\\sigma^2} (x_i - \\mu)\n\\end{aligned}\n$$\n\nSetting the derivative equal to zero,\n\n\n$$\n\\begin{aligned}\n\\sum_i \\left[ x_i - \\hat{\\mu} \\right] &= 0\\\\\nn \\hat{\\mu} &= \\sum_i x_i\\\\\n\\hat{\\mu} &= \\bar{x}\n\\end{aligned}\n$$\n\n\n## Exercise: binomial MLE\n\nLet $Y_1, Y_2,  \\ldots, Y_n \\sim \\text{iid } \\text{binary}(\\theta)$. Here $\\theta$ is the probability of a success, i.e. $prob(Y_i = 1)$. \n\n- Note: a \"binary\" distribution is equivalent to a \"Bernoulli\" distribution. Your book calls it \"binary\", so we will as well for consistency.\n\n1. Write down the likelihood, $p(y_1, \\ldots, y_n | \\theta)$.\n\n2. Write down the log-likelihood.\n\n3. Compute $\\hat{\\theta}_{MLE} = \\text{argmax}_{\\theta}~ \\log p(y_1, \\ldots, y_n | \\theta)$\n}\n\n## Solution\n\n::: panel-tabset\n## solution 1\n\n\n$$\n\\begin{aligned}\np(y_1, \\ldots, y_n | \\theta) &= \\prod_{i=1}^n p(y_i | \\theta)\\\\\n&= \\theta^{y_i}(1-\\theta)^{1-y_i}\\\\\n&= \\theta^{\\sum {y_i}}(1-\\theta)^{n - \\sum y_i}\n\\end{aligned}\n$$\n\n\n## solution 2\n\n\n$$\n\\log \\left(\\theta^{\\sum {y_i}}(1-\\theta)^{n - \\sum y_i} \\right) = \nn \\bar{y} \\log \\theta + n(1-\\bar{y}) \\log(1 - \\theta)\n$$\n\n\nwhere $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$.\n\n## solution 3\n\nTake the derivative \n\n$$\n\\begin{aligned}\n\\frac{d}{d\\theta} \\log p(y_1, \\ldots, y_n | \\theta) &=\n\\frac{n\\bar{y}}{\\theta} - \\frac{n - n\\bar{y})}{1- \\theta}\n\\end{aligned}\n$$\n\n\nand set equal to 0. After simplifying, \n\n\n$$\n\\hat{\\theta}_{MLE} = \\bar{y}\n$$\n\n:::\n\n<!-- # MAP -->\n\n<!-- ## Maximum a posteriori probability (MAP) -->\n\n<!-- In Bayesian inference, we wish to find the mode of the **posterior**, not the likelihood. -->\n\n<!-- To find the posterior mode, $\\hat{\\theta}$, we instead take the derivative of the *log-posterior*, -->\n\n<!-- $$ -->\n<!-- \\frac{d}{d\\theta} \\log p(\\theta | y) = 0 -->\n<!-- $$ -->\n\n<!-- ### Practice exercise -->\n\n<!-- As [in class](/notes/lec02-estimation.html), let -->\n\n<!-- $$ -->\n<!-- Y | \\theta \\sim \\text{binomial}(n, \\theta)\\\\ -->\n<!-- \\theta \\sim \\text{beta}(a, b) -->\n<!-- $$ -->\n\n<!-- 1. Find the closed-form solution for the posterior mode $\\hat{\\theta}$. -->\n\n<!-- 2. Recreate Figure 1 [from class](/notes/estimation1.html) using the *same data* `flips` provided below but change the prior to $\\theta \\sim \\text{beta}(2, 2)$. -->\n\n<!-- ```{r} -->\n<!-- set.seed(3) -->\n<!-- flips = rbinom(5000, size = 1, prob = 0.25) -->\n<!-- ``` -->\n\n\n<!-- 3. Add a red vertical line to each subplot that shows the MAP estimate under the prior $\\theta \\sim \\text{beta}(2, 2)$. -->\n\n",
    "supporting": [
      "lab1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}