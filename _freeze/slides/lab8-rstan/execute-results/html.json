{
  "hash": "6f4f2f3219a95fd1415838b35d789d54",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Easy Bayesian linear modeling\"\nformat: \n    revealjs:\n      mainfont: Lato\n      smaller: true\n---\n\n\n\n# `rstanarm` and `bayesplot`\n\n## Download\n\nTo download `rstanarm` and `bayesplot` run the code below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"rstanarm\", \"bayesplot\")\n```\n:::\n\n\n\nTo load the packages, run \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(bayesplot)\n```\n:::\n\n\n\n\n## Overview\n\n- `rstanarm` contains a host of functions to make Bayesian linear modeling in R easy. See [https://mc-stan.org/rstanarm/articles/](https://mc-stan.org/rstanarm/articles/) for a variety of tutorials.\n\n  - pros: easy to test Bayesian linear models, can be fast (uses Hamiltonian Monte Carlo proposals)\n  \n  - cons: limited in scope, e.g. requires differentiable objective and small model adjustments can be cumbersome to implement, e.g. placing a  prior on variance versus standard deviation of normal model.\n\n- `bayesplot` contains many useful plotting wrappers that work out of the box with objects created by `rstanarm` in an intuitive way.\n\n## Example\n\n:::panel-tabset\n\n## Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nspam = read_csv(\n  \"https://sta602-sp25.github.io/data/spam.csv\")\n```\n:::\n\n\n\n## Glimpse data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(spam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,601\nColumns: 58\n$ make              <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ address           <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ all               <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46…\n$ num3d             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ our               <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61…\n$ over              <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ remove            <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30…\n$ internet          <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00…\n$ order             <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92…\n$ mail              <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76…\n$ receive           <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76…\n$ will              <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92…\n$ people            <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00…\n$ report            <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ addresses         <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ free              <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00…\n$ business          <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ email             <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15…\n$ you               <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23…\n$ credit            <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53…\n$ your              <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00…\n$ font              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num000            <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ money             <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ hp                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hpl               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ george            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num650            <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ lab               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labs              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ telnet            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num857            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ data              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ num415            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num85             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ technology        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ num1999           <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ parts             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pm                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ direct            <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ cs                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ meeting           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ original          <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30…\n$ project           <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ re                <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ edu               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ table             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ conference        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ charSemicolon     <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charRoundbracket  <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2…\n$ charSquarebracket <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charExclamation   <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0…\n$ charDollar        <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0…\n$ charHash          <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ capitalAve        <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4…\n$ capitalLong       <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,…\n$ capitalTotal      <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2…\n$ type              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n:::\n\n\n:::\n\n**Description**\n\n4601 emails sent to the inbox of someone named \"George\" that are classified as `type` = 1 (spam) or 0 (non-spam). The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click [here](https://rdrr.io/cran/kernlab/man/spam.html) to read more.\n\n## Exercise Overview\n\nYou want to build a spam filter that blocks emails that have a high probability of being spam.\n\nIn statistical terms, your outcome $Y$ is the type of the email (spam or not). The 57 predictors from the data set are contained in $x$ and include: the frequency of certain words, the occurrence of certain symbols and the use of capital letters in each email.\n\nLet $X = \\{x_1, \\ldots, x_n\\}$ where $x_i \\in \\mathbb{R}^{58}$ (number of predictors + 1 for intercept) and $n =$ the number of emails in the data set. $Y \\in \\mathbb{R}^n$.\n\n$$\n\\begin{aligned}\np(y_i =1 | x_i, \\beta) = \\theta_i\\\\\n\\text{logit}(\\theta_i) = X\\beta\n\\end{aligned}\n$$\n\nA priori, you believe that many of the predictors included in the data set do not in fact help you predict whether the email is spam. You express your beliefs with the prior designated below:\n\n::: panel-tabset\n\n## Prior on intercept\n\nLet $\\beta_0$ be the intercept term.\n\n$$\n\\beta_0 \\sim Normal(0, 100)\n$$\n\n## Prior on the rest\n\nFor each $\\beta$ associated with the 57 predictors:\n\n$$\n\\beta_i \\sim \\text{iid}~Laplace(0, .5) \\ \\ \\text{for }i \\in \\{1, 57\\}{}\n$$\n:::\n\n## Standardize the data \n\n:::panel-tabset\n\n## Standardize data\n\n- Before we fit our model to the data, we need to standardize the predictors (columns of $X$). Why is this important? Discuss.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale functions re-scales columns of a df\nspam2 = cbind(\"type\" = spam$type, \n              scale(select(spam, -\"type\"))) %>%\n  data.frame()\n```\n:::\n\n\n\n## Training set\n\nTo validate our model we will separate it into non-overlapping sets -- a training set and a testing set. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(360) # ensures we get the same subset\nN = nrow(spam2)\nindices = sample(N, size = 0.8 * N)\nspam_train = spam2[indices,]\nspam_test = spam2[-indices,]\n# sanity check \nnrow(spam_train) + nrow(spam_test) == N\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Exercise 1 \n\nRead about how to fit Bayesian logistic regression using `rstanarm` here: [https://mc-stan.org/rstanarm/articles/binomial.html](https://mc-stan.org/rstanarm/articles/binomial.html) and write code to fit the `spam_train` data set. \n\nHint: use the `stan_glm` function. If you set arguments  `chains = 1`, this will run 1 Markov chain instead of the default 4. You can use the argument `iter=2000` to manually set the number of iterations in your Markov chain to 2000. This may take anywhere from 1-4 minutes to run locally on your machine. If you are pressed for time, you can load the resulting object directly from the website using the code below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 = readRDS(url(\"https://sta602-sp25.github.io/data/spam-train-fit.rds\"))\n```\n:::\n\n\n\n## Exercise 2\n\n**Examining the output**\n\n- Did `stan_glm` do what we think it did? Did the Markov chain converge? Which parameters, if any, have a 90% credible interval that covers 0?\n\n::: panel-tabset\n\n## quick look\n\nNotice `sample: 1000` since half get thrown away by `stan` and is called \"burn-in\" i.e. a period that the chain spends reaching the target distribution gets discarded.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      type ~ .\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 3680\n predictors:   58\n\nEstimates:\n                    mean   sd    10%   50%   90%\n(Intercept)        -3.9    0.5  -4.5  -3.9  -3.4\nmake               -0.1    0.1  -0.2  -0.1   0.0\naddress            -0.2    0.1  -0.4  -0.2  -0.1\nall                 0.1    0.1   0.0   0.1   0.1\nnum3d               0.9    0.7   0.2   0.7   1.7\nour                 0.4    0.1   0.3   0.4   0.5\nover                0.2    0.1   0.1   0.2   0.3\nremove              0.9    0.1   0.7   0.9   1.1\ninternet            0.2    0.1   0.1   0.2   0.3\norder               0.1    0.1   0.0   0.1   0.2\nmail                0.1    0.0   0.0   0.1   0.1\nreceive             0.0    0.1  -0.1   0.0   0.0\nwill               -0.2    0.1  -0.3  -0.2  -0.1\npeople             -0.1    0.1  -0.1   0.0   0.0\nreport              0.0    0.1  -0.1   0.0   0.1\naddresses           0.3    0.2   0.1   0.3   0.5\nfree                1.0    0.1   0.8   1.0   1.1\nbusiness            0.5    0.1   0.3   0.5   0.6\nemail               0.1    0.1   0.0   0.1   0.2\nyou                 0.1    0.1   0.0   0.1   0.2\ncredit              0.6    0.2   0.3   0.6   0.9\nyour                0.3    0.1   0.2   0.3   0.4\nfont                0.2    0.2   0.1   0.2   0.4\nnum000              0.7    0.2   0.5   0.7   0.9\nmoney               0.2    0.1   0.1   0.2   0.3\nhp                 -3.1    0.5  -3.8  -3.1  -2.5\nhpl                -1.0    0.4  -1.5  -1.0  -0.5\ngeorge             -8.5    1.8 -10.9  -8.5  -6.3\nnum650              0.2    0.1   0.1   0.2   0.4\nlab                -1.1    0.5  -1.8  -1.0  -0.5\nlabs               -0.1    0.1  -0.3  -0.1   0.0\ntelnet             -0.3    0.3  -0.8  -0.3   0.0\nnum857             -0.2    0.6  -1.0  -0.2   0.4\ndata               -0.7    0.2  -1.0  -0.7  -0.5\nnum415             -0.6    0.8  -1.7  -0.5   0.2\nnum85              -0.8    0.4  -1.3  -0.8  -0.3\ntechnology          0.4    0.1   0.2   0.4   0.6\nnum1999             0.0    0.1  -0.1   0.0   0.1\nparts              -0.2    0.1  -0.4  -0.2  -0.1\npm                 -0.4    0.2  -0.6  -0.4  -0.2\ndirect             -0.2    0.1  -0.3  -0.1   0.0\ncs                 -1.4    0.8  -2.4  -1.2  -0.5\nmeeting            -1.5    0.5  -2.2  -1.5  -1.0\noriginal           -0.4    0.2  -0.8  -0.4  -0.1\nproject            -1.3    0.4  -1.8  -1.3  -0.8\nre                 -0.7    0.2  -0.9  -0.7  -0.5\nedu                -1.5    0.3  -1.9  -1.5  -1.1\ntable              -0.3    0.1  -0.5  -0.3  -0.1\nconference         -1.1    0.4  -1.7  -1.1  -0.6\ncharSemicolon      -0.3    0.1  -0.5  -0.3  -0.2\ncharRoundbracket   -0.1    0.1  -0.2  -0.1   0.0\ncharSquarebracket  -0.1    0.1  -0.3  -0.1   0.0\ncharExclamation     0.2    0.1   0.2   0.2   0.3\ncharDollar          1.2    0.2   1.0   1.3   1.5\ncharHash            0.7    0.4   0.2   0.7   1.2\ncapitalAve          0.0    0.3  -0.3   0.0   0.4\ncapitalLong         0.9    0.4   0.4   0.8   1.3\ncapitalTotal        0.7    0.1   0.5   0.7   0.8\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.4   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                  mcse Rhat n_eff\n(Intercept)       0.0  1.0   740 \nmake              0.0  1.0  1161 \naddress           0.0  1.0  1156 \nall               0.0  1.0   907 \nnum3d             0.0  1.0   958 \nour               0.0  1.0   973 \nover              0.0  1.0  1022 \nremove            0.0  1.0   909 \ninternet          0.0  1.0  1188 \norder             0.0  1.0  1064 \nmail              0.0  1.0  1022 \nreceive           0.0  1.0  1171 \nwill              0.0  1.0  1022 \npeople            0.0  1.0  1307 \nreport            0.0  1.0   935 \naddresses         0.0  1.0  1115 \nfree              0.0  1.0   830 \nbusiness          0.0  1.0  1059 \nemail             0.0  1.0   895 \nyou               0.0  1.0  1026 \ncredit            0.0  1.0  1328 \nyour              0.0  1.0  1077 \nfont              0.0  1.0  1226 \nnum000            0.0  1.0  1124 \nmoney             0.0  1.0   910 \nhp                0.0  1.0  1161 \nhpl               0.0  1.0  1422 \ngeorge            0.1  1.0   779 \nnum650            0.0  1.0  1017 \nlab               0.0  1.0  1477 \nlabs              0.0  1.0  1317 \ntelnet            0.0  1.0  1002 \nnum857            0.0  1.0  1388 \ndata              0.0  1.0  1265 \nnum415            0.0  1.0  1427 \nnum85             0.0  1.0  1247 \ntechnology        0.0  1.0  1021 \nnum1999           0.0  1.0   834 \nparts             0.0  1.0  1227 \npm                0.0  1.0  1135 \ndirect            0.0  1.0   793 \ncs                0.0  1.0  1293 \nmeeting           0.0  1.0  1547 \noriginal          0.0  1.0  1805 \nproject           0.0  1.0  1432 \nre                0.0  1.0  1027 \nedu               0.0  1.0  1048 \ntable             0.0  1.0  1082 \nconference        0.0  1.0  1492 \ncharSemicolon     0.0  1.0  1308 \ncharRoundbracket  0.0  1.0   906 \ncharSquarebracket 0.0  1.0  1286 \ncharExclamation   0.0  1.0  1070 \ncharDollar        0.0  1.0  1147 \ncharHash          0.0  1.0  1111 \ncapitalAve        0.0  1.0  1165 \ncapitalLong       0.0  1.0  1421 \ncapitalTotal      0.0  1.0  1312 \nmean_PPD          0.0  1.0   909 \nlog-posterior     0.5  1.0   338 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## check priors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 10)\n\nCoefficients\n ~ laplace(location = [0,0,0,...], scale = [0.5,0.5,0.5,...])\n------\nSee help('prior_summary.stanreg') for more details\n```\n\n\n:::\n:::\n\n\n## trace plots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbetaNames = names(spam_train)[2:7]\nbetaNames\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc_trace(fit1, pars = betaNames)\n```\n\n::: {.cell-output-display}\n![](lab8-rstan_files/figure-revealjs/unnamed-chunk-10-1.png){width=480}\n:::\n:::\n\n\n## marginal posteriors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbetaNames = names(spam_train)[2:7]\nbetaNames\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n```\n\n\n:::\n\n```{.r .cell-code}\nmcmc_hist(fit1, pars = c(betaNames))\n```\n\n::: {.cell-output-display}\n![](lab8-rstan_files/figure-revealjs/unnamed-chunk-11-1.png){width=480}\n:::\n:::\n\n\n## plotting tips\n\nTo plot specific parameters, use the arguemnt `pars`, e.g.\n\n- `mcmc_trace(fit1, pars = c(\"internet\", \"george\")`\n- `mcmc_hist(fit1, pars = \"make\")`\n\nTo read more about `bayesplot` functionality, see [https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)\n\n<!-- ## residual plot -->\n\n<!-- ```{r} -->\n<!-- #| fig-width: 5 -->\n<!-- #| fig-height: 3 -->\n<!-- df = data.frame(yhat = fit1$fitted.values, -->\n<!--                 residual = fit1$residuals) -->\n\n<!-- df %>% -->\n<!--   ggplot(aes(x = yhat, y = residual)) + -->\n<!--   geom_point() + -->\n<!--   theme_bw() -->\n\n<!-- ``` -->\n## get chain\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchain_draws = as_draws(fit1)\nchain_draws$george[1:5] # first 5 samples of the first chain run by stan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  -8.657706  -8.058416  -7.542682  -5.875524 -10.805427\n```\n\n\n:::\n:::\n\n\n\n- try the following command: `View(chain_draws)`\n\n## summarize\n\nReport posterior mean, posterior median and 90% posterior CI.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposteriorMean = apply(chain_draws, 2, mean)\nposteriorMedian = fit1$coefficients\nposteriorCI = posterior_interval(fit1, prob = 0.9)\ncbind(posteriorMean, posteriorMedian, posteriorCI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  posteriorMean posteriorMedian            5%          95%\n(Intercept)         -3.92819676     -3.89842329  -4.688486640 -3.214996079\nmake                -0.10511093     -0.10291579  -0.234925630  0.007473911\naddress             -0.23994990     -0.22496984  -0.459838123 -0.079085239\nall                  0.06181680      0.06204777  -0.032348802  0.162371165\nnum3d                0.86239227      0.69068831   0.094477008  2.262920466\nour                  0.36326097      0.36517847   0.252660614  0.482222431\nover                 0.22388788      0.22526813   0.104903413  0.345376589\nremove               0.89046939      0.88519560   0.660560624  1.124715173\ninternet             0.19219955      0.18873972   0.084482068  0.313373050\norder                0.14088547      0.14175852   0.022540540  0.262790934\nmail                 0.07091792      0.07049483  -0.004812217  0.148380741\nreceive             -0.04641076     -0.04704090  -0.155648064  0.056349226\nwill                -0.19669579     -0.19506749  -0.323302246 -0.079179801\npeople              -0.05080340     -0.04939546  -0.170276558  0.067469609\nreport              -0.01152332     -0.01105396  -0.102076144  0.078218538\naddresses            0.27381609      0.26410052   0.016663808  0.554382803\nfree                 0.95909363      0.95496752   0.738679361  1.184988932\nbusiness             0.46684266      0.46497563   0.276894271  0.663670058\nemail                0.10488455      0.10024984   0.001909845  0.222419640\nyou                  0.10146793      0.10210904  -0.005066755  0.212057793\ncredit               0.60260737      0.58770051   0.245466733  1.011651069\nyour                 0.29510195      0.29496631   0.185160138  0.408134909\nfont                 0.23712284      0.22816521   0.014405002  0.514370136\nnum000               0.71052738      0.69647322   0.470113449  0.973663603\nmoney                0.20621767      0.19864778   0.101091758  0.340661983\nhp                  -3.12311740     -3.10880324  -4.016671505 -2.304686191\nhpl                 -1.03485238     -1.03298227  -1.697441864 -0.401214421\ngeorge              -8.54020532     -8.47338329 -11.651960267 -5.719147532\nnum650               0.23079106      0.22722517   0.066179720  0.417323354\nlab                 -1.06217022     -0.98480464  -2.007243002 -0.334531315\nlabs                -0.12552567     -0.11162875  -0.398538494  0.091786582\ntelnet              -0.34336793     -0.26439840  -0.971412117  0.026123689\nnum857              -0.24559769     -0.15437261  -1.410789083  0.608907729\ndata                -0.73133589     -0.71699338  -1.108487928 -0.382954624\nnum415              -0.64101304     -0.47534569  -2.093090996  0.285482781\nnum85               -0.79569146     -0.76847847  -1.439452875 -0.215331103\ntechnology           0.39271940      0.38676108   0.173703662  0.625821033\nnum1999             -0.01626319     -0.01501346  -0.146351106  0.113311375\nparts               -0.19387536     -0.17594695  -0.420298806 -0.034525634\npm                  -0.36307678     -0.35286882  -0.654982493 -0.099008434\ndirect              -0.16079193     -0.14842066  -0.405108058  0.023707001\ncs                  -1.35347826     -1.21771081  -2.748096259 -0.377142268\nmeeting             -1.54394154     -1.48634297  -2.451147222 -0.853955898\noriginal            -0.44492327     -0.43068267  -0.889558420 -0.068304526\nproject             -1.27091449     -1.25700373  -1.901896255 -0.691331691\nre                  -0.68897480     -0.68667471  -0.941480034 -0.439694059\nedu                 -1.52342346     -1.51146322  -2.056088445 -1.041884151\ntable               -0.27888780     -0.25736694  -0.549891396 -0.077155657\nconference          -1.10733935     -1.06265166  -1.864536810 -0.460671052\ncharSemicolon       -0.32588657     -0.31416332  -0.514101364 -0.166688438\ncharRoundbracket    -0.08698799     -0.08292353  -0.212643020  0.023441788\ncharSquarebracket   -0.14980893     -0.13280261  -0.342135625 -0.004178006\ncharExclamation      0.23238206      0.22752584   0.137507390  0.351658871\ncharDollar           1.24928427      1.25351495   0.944702156  1.558125382\ncharHash             0.71795213      0.70213467   0.122922001  1.368832748\ncapitalAve          -0.01033156     -0.03262611  -0.401092859  0.470713740\ncapitalLong          0.85197631      0.82214223   0.303300262  1.493571408\ncapitalTotal         0.67337732      0.67245181   0.471182694  0.893198052\n```\n\n\n:::\n:::\n\n\n:::\n\n## Exercise 3\n\n- Test your spam filter on the `spam_test` data set.\n\n- Make a table showing correct and incorrect number of classifications.\n\n\n# Solutions\n\n## Exercise 1 solution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 = stan_glm(type ~ ., data = spam_train,\n                 family = binomial(link = \"logit\"),\n                 prior = laplace(0, 0.5),\n                 prior_intercept = normal(0, 10),\n                 cores = 2, seed = 360,\n                chains = 1, iter = 2000)\n```\n:::\n\n\n\n## Exercise 2 Solution\n\n- Trace plots look good, can look through more by subsetting others.\n\n- ESS is high\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich(sign(posteriorCI[,1]) != sign(posteriorCI[,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            make              all             mail          receive \n               2                4               11               12 \n          people           report              you             labs \n              14               15               20               31 \n          telnet           num857           num415          num1999 \n              32               33               35               38 \n          direct charRoundbracket       capitalAve \n              41               51               56 \n```\n\n\n:::\n:::\n\n\n\n<!-- - `receive`, `report`, `num1999`, `capitalAve`  -->\n\n## Exercise 3 Solution\n\n<!-- predicted_spam = posterior_predict(object = fit1, newdata = spam_test) -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(y = spam_test$type, \n           yhat = predict(object = fit1, newdata = spam_test[,-1], type = \"response\")) %>%\n  mutate(yhat = ifelse(yhat >= 0.5, 1, 0)) %>%\n  count(y, yhat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  y yhat   n\n1 0    0 529\n2 0    1  26\n3 1    0  31\n4 1    1 335\n```\n\n\n:::\n:::\n\n\n\n856/921 classifications correct with a cutoff of 0.5. \n\n",
    "supporting": [
      "lab8-rstan_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}